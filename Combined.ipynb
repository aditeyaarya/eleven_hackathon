{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4f87ea-e7a6-4721-ac69-411076c60e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 0) Imports\n",
    "# ---------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5395cfbd-7dfd-4dff-af80-0ae012e57bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes:\n",
      " clients: (424037, 7)\n",
      " products: (47458, 5)\n",
      " transactions: (1177175, 6)\n",
      " stocks: (16024, 3)\n",
      " stores: (606, 2)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1) Paths + Load data\n",
    "# ---------------------------\n",
    "CLIENTS_PATH = \"data/clients.csv\"\n",
    "PRODUCTS_PATH = \"data/products.csv\"\n",
    "TRANSACTIONS_PATH = \"data/transactions.csv\"\n",
    "STOCKS_PATH = \"data/stocks.csv\"\n",
    "STORES_PATH = \"data/stores.csv\"  # optional\n",
    "\n",
    "MODEL_DIR = \"recommender_artifacts\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "clients = pd.read_csv(CLIENTS_PATH)\n",
    "products = pd.read_csv(PRODUCTS_PATH)\n",
    "transactions = pd.read_csv(TRANSACTIONS_PATH)\n",
    "stocks = pd.read_csv(STOCKS_PATH) if os.path.exists(STOCKS_PATH) else None\n",
    "stores = pd.read_csv(STORES_PATH) if os.path.exists(STORES_PATH) else None\n",
    "\n",
    "# Standardize dtypes\n",
    "clients[\"ClientID\"] = clients[\"ClientID\"].astype(str)\n",
    "products[\"ProductID\"] = products[\"ProductID\"].astype(str)\n",
    "transactions[\"ClientID\"] = transactions[\"ClientID\"].astype(str)\n",
    "transactions[\"ProductID\"] = transactions[\"ProductID\"].astype(str)\n",
    "\n",
    "# Transaction fields\n",
    "transactions[\"SaleTransactionDate\"] = pd.to_datetime(transactions[\"SaleTransactionDate\"], errors=\"coerce\")\n",
    "transactions[\"Quantity\"] = pd.to_numeric(transactions.get(\"Quantity\", 1), errors=\"coerce\").fillna(1.0).clip(lower=0)\n",
    "transactions[\"SalesNetAmountEuro\"] = pd.to_numeric(transactions.get(\"SalesNetAmountEuro\", 0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Stocks\n",
    "if stocks is not None:\n",
    "    stocks[\"ProductID\"] = stocks[\"ProductID\"].astype(str)\n",
    "    stocks[\"Quantity\"] = pd.to_numeric(stocks.get(\"Quantity\", 0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "print(\"Loaded shapes:\")\n",
    "print(\" clients:\", clients.shape)\n",
    "print(\" products:\", products.shape)\n",
    "print(\" transactions:\", transactions.shape)\n",
    "print(\" stocks:\", None if stocks is None else stocks.shape)\n",
    "print(\" stores:\", None if stores is None else stores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8314c3e-4adf-425e-9869-530cb98b417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Utilities\n",
    "# ---------------------------\n",
    "FEATURES_HIER = [\"Category\", \"FamilyLevel1\", \"Universe\", \"FamilyLevel2\"]\n",
    "for c in FEATURES_HIER:\n",
    "    if c not in products.columns:\n",
    "        raise ValueError(f\"products.csv missing required column: {c}\")\n",
    "\n",
    "def get_in_stock_ids():\n",
    "    if stocks is None:\n",
    "        return set(products[\"ProductID\"].unique())\n",
    "    return set(stocks.loc[stocks[\"Quantity\"] > 0, \"ProductID\"].unique())\n",
    "\n",
    "IN_STOCK_IDS = get_in_stock_ids()\n",
    "\n",
    "def make_content_key(df):\n",
    "    return df[[\"Category\", \"FamilyLevel1\", \"FamilyLevel2\", \"Universe\"]].fillna(\"\").agg(\" | \".join, axis=1)\n",
    "\n",
    "def exp_decay(days_ago, half_life_days):\n",
    "    return np.exp(-days_ago / float(half_life_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c54d7a-6e00-4b18-a134-08751bccdc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded content model from disk.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART A — CONTENT-BASED PRODUCT SIMILARITY MODEL (build/save/load)\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Build weighted content similarity (Category > FamilyLevel1 > Universe > FamilyLevel2)\n",
    "# ---------------------------\n",
    "def build_content_similarity(products_df, weights=(0.50, 0.30, 0.20)):\n",
    "    \"\"\"\n",
    "    Builds similarity matrix with explicit weighting:\n",
    "      S = w_cat*S_cat + w_fam1*S_fam1 + w_other*S_other\n",
    "    where \"other\" uses Universe + FamilyLevel2 tokens.\n",
    "    \"\"\"\n",
    "    w_cat, w_fam1, w_other = weights\n",
    "\n",
    "    df = products_df.copy()\n",
    "    df[\"ProductID\"] = df[\"ProductID\"].astype(str)\n",
    "\n",
    "    # Text blocks\n",
    "    df[\"cat_text\"] = df[\"Category\"].fillna(\"\")\n",
    "    df[\"fam1_text\"] = df[\"FamilyLevel1\"].fillna(\"\")\n",
    "    df[\"other_text\"] = (df[\"Universe\"].fillna(\"\") + \" \" + df[\"FamilyLevel2\"].fillna(\"\"))\n",
    "\n",
    "    vec_cat = TfidfVectorizer(lowercase=True)\n",
    "    vec_fam1 = TfidfVectorizer(lowercase=True)\n",
    "    vec_other = TfidfVectorizer(lowercase=True, ngram_range=(1, 2))\n",
    "\n",
    "    X_cat = vec_cat.fit_transform(df[\"cat_text\"])\n",
    "    X_fam1 = vec_fam1.fit_transform(df[\"fam1_text\"])\n",
    "    X_other = vec_other.fit_transform(df[\"other_text\"])\n",
    "\n",
    "    S_cat = cosine_similarity(X_cat, X_cat)\n",
    "    S_fam1 = cosine_similarity(X_fam1, X_fam1)\n",
    "    S_other = cosine_similarity(X_other, X_other)\n",
    "\n",
    "    S = w_cat * S_cat + w_fam1 * S_fam1 + w_other * S_other\n",
    "\n",
    "    # Index mapping\n",
    "    product_index = pd.Series(df.index, index=df[\"ProductID\"])\n",
    "\n",
    "    # content_key used later for dedup\n",
    "    df[\"content_key\"] = make_content_key(df)\n",
    "\n",
    "    return {\n",
    "        \"products_model\": df,\n",
    "        \"similarity_matrix\": S,\n",
    "        \"product_index\": product_index,\n",
    "        \"weights\": {\"W_CAT\": w_cat, \"W_FAM1\": w_fam1, \"W_OTHER\": w_other},\n",
    "        \"vectorizers\": {\"vec_cat\": vec_cat, \"vec_fam1\": vec_fam1, \"vec_other\": vec_other},\n",
    "    }\n",
    "\n",
    "def save_content_model(model_bundle, model_dir=MODEL_DIR):\n",
    "    joblib.dump(model_bundle[\"products_model\"], f\"{model_dir}/products_model.joblib\")\n",
    "    joblib.dump(model_bundle[\"similarity_matrix\"], f\"{model_dir}/similarity_matrix.joblib\")\n",
    "    joblib.dump(model_bundle[\"product_index\"], f\"{model_dir}/product_index.joblib\")\n",
    "    joblib.dump(model_bundle[\"weights\"], f\"{model_dir}/weights.joblib\")\n",
    "    # Vectorizers optional for inference if you keep similarity_matrix, but save anyway\n",
    "    joblib.dump(model_bundle[\"vectorizers\"][\"vec_cat\"], f\"{model_dir}/vec_cat.joblib\")\n",
    "    joblib.dump(model_bundle[\"vectorizers\"][\"vec_fam1\"], f\"{model_dir}/vec_fam1.joblib\")\n",
    "    joblib.dump(model_bundle[\"vectorizers\"][\"vec_other\"], f\"{model_dir}/vec_other.joblib\")\n",
    "\n",
    "def load_content_model(model_dir=MODEL_DIR):\n",
    "    required = [\"products_model.joblib\", \"similarity_matrix.joblib\", \"product_index.joblib\", \"weights.joblib\"]\n",
    "    if not all(os.path.exists(f\"{model_dir}/{f}\") for f in required):\n",
    "        return None\n",
    "\n",
    "    products_model = joblib.load(f\"{model_dir}/products_model.joblib\")\n",
    "    similarity_matrix = joblib.load(f\"{model_dir}/similarity_matrix.joblib\")\n",
    "    product_index = joblib.load(f\"{model_dir}/product_index.joblib\")\n",
    "    weights = joblib.load(f\"{model_dir}/weights.joblib\")\n",
    "    return {\n",
    "        \"products_model\": products_model,\n",
    "        \"similarity_matrix\": similarity_matrix,\n",
    "        \"product_index\": product_index,\n",
    "        \"weights\": weights,\n",
    "    }\n",
    "\n",
    "# Build or load content model (built on in-stock catalog snapshot)\n",
    "content_model = load_content_model(MODEL_DIR)\n",
    "if content_model is None:\n",
    "    products_in_stock = products[products[\"ProductID\"].isin(IN_STOCK_IDS)].copy()\n",
    "    content_model = build_content_similarity(products_in_stock, weights=(0.50, 0.30, 0.20))\n",
    "    save_content_model(content_model, MODEL_DIR)\n",
    "    print(\"✅ Built + saved content model.\")\n",
    "else:\n",
    "    print(\"✅ Loaded content model from disk.\")\n",
    "\n",
    "products_model = content_model[\"products_model\"]\n",
    "S = content_model[\"similarity_matrix\"]\n",
    "product_index_model = content_model[\"product_index\"]\n",
    "\n",
    "\n",
    "def content_candidates_from_seed(seed_product_id, per_seed_k=80):\n",
    "    \"\"\"\n",
    "    Return candidates similar to a seed product from the saved similarity model.\n",
    "    Output includes ProductID + FEATURES_HIER + sim_score + content_key\n",
    "    \"\"\"\n",
    "    pid = str(seed_product_id)\n",
    "    if pid not in product_index_model:\n",
    "        return pd.DataFrame(columns=[\"ProductID\", \"sim_score\"] + FEATURES_HIER + [\"content_key\"])\n",
    "\n",
    "    idx = product_index_model[pid]\n",
    "    # idx is label index of products_model; need row position in similarity matrix\n",
    "    row_pos = products_model.index.get_loc(idx)\n",
    "\n",
    "    sims = S[row_pos]\n",
    "    sim_scores = pd.Series(sims, index=products_model.index).sort_values(ascending=False)\n",
    "\n",
    "    # exclude itself\n",
    "    sim_scores = sim_scores.drop(index=idx, errors=\"ignore\")\n",
    "\n",
    "    top_idx = sim_scores.head(per_seed_k).index\n",
    "    out = products_model.loc[top_idx, [\"ProductID\"] + FEATURES_HIER + [\"content_key\"]].copy()\n",
    "    out[\"sim_score\"] = sim_scores.loc[top_idx].values\n",
    "    return out.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c58d338-dab2-4330-8f91-fc097ecb77dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rows with next target: 872246\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART B — PREDICTION MODEL (XGBOOST) FOR NEXT FamilyLevel2\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Build modeling table with a true \"next purchase\" target\n",
    "# ---------------------------\n",
    "tx = transactions.merge(products[[\"ProductID\"] + FEATURES_HIER], on=\"ProductID\", how=\"left\")\n",
    "\n",
    "client_cols = [c for c in [\"ClientSegment\", \"ClientCountry\", \"ClientGender\", \"Age\", \"ClientOptINEmail\", \"ClientOptINPhone\"] if c in clients.columns]\n",
    "tx = tx.merge(clients[[\"ClientID\"] + client_cols], on=\"ClientID\", how=\"left\")\n",
    "\n",
    "if stores is not None and \"StoreID\" in tx.columns and \"StoreID\" in stores.columns and \"StoreCountry\" in stores.columns:\n",
    "    tx = tx.merge(stores[[\"StoreID\", \"StoreCountry\"]], on=\"StoreID\", how=\"left\")\n",
    "\n",
    "tx = tx.sort_values([\"ClientID\", \"SaleTransactionDate\"])\n",
    "tx[\"TargetNextFamilyLevel2\"] = tx.groupby(\"ClientID\")[\"FamilyLevel2\"].shift(-1)\n",
    "\n",
    "df_model = tx.dropna(subset=[\"TargetNextFamilyLevel2\"]).copy()\n",
    "df_model = df_model[df_model[\"SaleTransactionDate\"].notna()].copy()\n",
    "\n",
    "print(\"Model rows with next target:\", df_model.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cf6057-b6c0-4156-ac1c-6eeaf4968c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Feature engineering (history up to time t)\n",
    "# ---------------------------\n",
    "HALF_LIFE_DAYS = 90.0  # recency decay for client history\n",
    "\n",
    "df_model[\"PrevDate\"] = df_model.groupby(\"ClientID\")[\"SaleTransactionDate\"].shift(1)\n",
    "df_model[\"RecencyDays\"] = (df_model[\"SaleTransactionDate\"] - df_model[\"PrevDate\"]).dt.days\n",
    "df_model[\"RecencyDays\"] = pd.to_numeric(df_model[\"RecencyDays\"], errors=\"coerce\")\n",
    "df_model[\"RecencyDays\"] = df_model[\"RecencyDays\"].fillna(df_model[\"RecencyDays\"].median())\n",
    "\n",
    "df_model[\"CumQty\"] = df_model.groupby(\"ClientID\")[\"Quantity\"].cumsum().shift(1).fillna(0.0)\n",
    "df_model[\"CumSpend\"] = df_model.groupby(\"ClientID\")[\"SalesNetAmountEuro\"].cumsum().shift(1).fillna(0.0)\n",
    "df_model[\"CumTxns\"] = df_model.groupby(\"ClientID\").cumcount()  # number of previous txns\n",
    "df_model[\"AvgBasket\"] = (df_model[\"CumSpend\"] / df_model[\"CumTxns\"].replace(0, np.nan)).fillna(0.0)\n",
    "\n",
    "# Last purchased attributes\n",
    "for col in [\"Category\", \"FamilyLevel1\", \"Universe\", \"FamilyLevel2\"]:\n",
    "    df_model[f\"Last_{col}\"] = df_model.groupby(\"ClientID\")[col].shift(1)\n",
    "\n",
    "# Top FamilyLevel1 affinity features (recency-weighted)\n",
    "top_fam1 = df_model[\"FamilyLevel1\"].value_counts().head(15).index.tolist()\n",
    "\n",
    "global_max_date = df_model[\"SaleTransactionDate\"].max()\n",
    "days_ago = (global_max_date - df_model[\"SaleTransactionDate\"]).dt.days.clip(lower=0).fillna(0)\n",
    "df_model[\"EventW\"] = exp_decay(days_ago, HALF_LIFE_DAYS) * df_model[\"Quantity\"]\n",
    "\n",
    "for fam in top_fam1:\n",
    "    m = (df_model[\"FamilyLevel1\"] == fam).astype(float)\n",
    "    colname = f\"CntFam1_{fam}\"\n",
    "    df_model[colname] = (m * df_model[\"EventW\"])\n",
    "    df_model[colname] = df_model.groupby(\"ClientID\")[colname].cumsum().shift(1).fillna(0.0)\n",
    "\n",
    "# Clean age\n",
    "if \"Age\" in df_model.columns:\n",
    "    df_model[\"Age\"] = pd.to_numeric(df_model[\"Age\"], errors=\"coerce\")\n",
    "    df_model[\"Age\"] = df_model[\"Age\"].fillna(df_model[\"Age\"].median() if df_model[\"Age\"].notna().any() else 0.0)\n",
    "else:\n",
    "    df_model[\"Age\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b0d47d-1274-40dd-8b28-5203e894c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Top-1: 0.1165 | Top-3: 0.2288 | Top-5: 0.3161\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6) Train XGBoost (FIXED with LabelEncoder)\n",
    "# ---------------------------\n",
    "num_features = [\"RecencyDays\", \"CumQty\", \"CumSpend\", \"CumTxns\", \"AvgBasket\", \"Age\"] + [f\"CntFam1_{fam}\" for fam in top_fam1]\n",
    "cat_features = [c for c in (\n",
    "    [\"ClientSegment\", \"ClientCountry\", \"ClientGender\", \"ClientOptINEmail\", \"ClientOptINPhone\"] +\n",
    "    [f\"Last_{x}\" for x in [\"Category\", \"FamilyLevel1\", \"Universe\", \"FamilyLevel2\"]]\n",
    ") if c in df_model.columns]\n",
    "\n",
    "X = df_model[num_features + cat_features].copy()\n",
    "y = df_model[\"TargetNextFamilyLevel2\"].astype(str)\n",
    "\n",
    "# Time-based split (last 20% by date as test)\n",
    "df_model = df_model.sort_values(\"SaleTransactionDate\")\n",
    "split_idx = int(len(df_model) * 0.8)\n",
    "train_idx = df_model.index[:split_idx]\n",
    "test_idx = df_model.index[split_idx:]\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "# Robustness: filter test to labels seen in train\n",
    "train_classes = set(y_train.unique())\n",
    "mask = y_test.isin(train_classes)\n",
    "X_test, y_test = X_test.loc[mask], y_test.loc[mask]\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "num_class = len(le.classes_)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
    "        (\"num\", \"passthrough\", num_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=num_class,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[(\"prep\", preprocess), (\"model\", xgb)])\n",
    "clf.fit(X_train, y_train_enc)\n",
    "\n",
    "probs = clf.predict_proba(X_test)\n",
    "labels = np.arange(num_class)  # 0..num_class-1\n",
    "\n",
    "top1 = top_k_accuracy_score(y_test_enc, probs, k=1, labels=labels)\n",
    "top3 = top_k_accuracy_score(y_test_enc, probs, k=3, labels=labels)\n",
    "top5 = top_k_accuracy_score(y_test_enc, probs, k=5, labels=labels)\n",
    "\n",
    "print(f\"XGB Top-1: {top1:.4f} | Top-3: {top3:.4f} | Top-5: {top5:.4f}\")\n",
    "\n",
    "# We'll decode predicted class indices using le.classes_\n",
    "xgb_classes = le.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5db1a6-954f-494d-aff5-ce58e119ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PART C — CLIENT PROFILE + FINAL TWO-STAGE RECOMMENDER\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Client profile builder (priority: Category > FamilyLevel1 > Universe > FamilyLevel2)\n",
    "# ---------------------------\n",
    "W_PROFILE = {\"Category\": 4.0, \"FamilyLevel1\": 2.5, \"Universe\": 1.5, \"FamilyLevel2\": 1.0}\n",
    "\n",
    "txp = transactions.merge(products[[\"ProductID\"] + FEATURES_HIER], on=\"ProductID\", how=\"left\")\n",
    "\n",
    "def build_client_profile(client_id, half_life_days=HALF_LIFE_DAYS):\n",
    "    cid = str(client_id)\n",
    "    df = txp[txp[\"ClientID\"].astype(str) == cid].copy()\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    max_date = txp[\"SaleTransactionDate\"].max()\n",
    "    days_ago = (max_date - df[\"SaleTransactionDate\"]).dt.days.clip(lower=0).fillna(0)\n",
    "    base_w = df[\"Quantity\"] * exp_decay(days_ago, half_life_days)\n",
    "\n",
    "    profile = {}\n",
    "    for feat in FEATURES_HIER:\n",
    "        s = df.groupby(feat).apply(lambda g: float(base_w.loc[g.index].sum()))\n",
    "        s = (s * W_PROFILE[feat]).sort_values(ascending=False)\n",
    "        profile[feat] = s\n",
    "    return profile\n",
    "\n",
    "def profile_boost(profile, candidates_df):\n",
    "    b = pd.Series(0.0, index=candidates_df.index)\n",
    "    if not profile:\n",
    "        return b\n",
    "    for feat in FEATURES_HIER:\n",
    "        pref = profile.get(feat)\n",
    "        if pref is None or pref.empty:\n",
    "            continue\n",
    "        b += candidates_df[feat].fillna(\"\").map(pref).fillna(0.0)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82021af2-a335-4aff-86b2-a1f797177e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8) Predict top-N families for a client (from latest available feature-row)\n",
    "# ---------------------------\n",
    "def get_latest_feature_row_for_client(client_id):\n",
    "    cid = str(client_id)\n",
    "    sub = df_model[df_model[\"ClientID\"].astype(str) == cid].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.sort_values(\"SaleTransactionDate\")\n",
    "    return sub.iloc[[-1]]\n",
    "\n",
    "def predict_top_families_for_client(client_id, top_n=5):\n",
    "    last_row = get_latest_feature_row_for_client(client_id)\n",
    "    if last_row is None:\n",
    "        return []\n",
    "    X_last = last_row[num_features + cat_features]\n",
    "    p = clf.predict_proba(X_last)[0]\n",
    "    top_idx = np.argsort(p)[::-1][:top_n]\n",
    "    fams = le.inverse_transform(top_idx)\n",
    "    probs_ = p[top_idx]\n",
    "    return list(zip(fams, probs_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37d1d63-2ed4-44a7-9f04-53123964ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 9) Final recommender: Candidate gen (XGB families) + Rerank (content + profile)\n",
    "# ---------------------------\n",
    "def recommend_for_client(\n",
    "    client_id,\n",
    "    top_k=10,\n",
    "    top_families_n=5,\n",
    "    n_seed_products=3,\n",
    "    per_seed_k=80,\n",
    "    dedup_by_content=True,\n",
    "):\n",
    "    cid = str(client_id)\n",
    "\n",
    "    # Already bought items\n",
    "    bought = set(transactions.loc[transactions[\"ClientID\"] == cid, \"ProductID\"].astype(str).unique())\n",
    "\n",
    "    # Candidate generation: predicted next families\n",
    "    fam_preds = predict_top_families_for_client(cid, top_n=top_families_n)\n",
    "    pred_fams = [f for f, _ in fam_preds]\n",
    "\n",
    "    # Candidate pool from products, filter constraints\n",
    "    cand = products.copy()\n",
    "    cand = cand[cand[\"ProductID\"].isin(IN_STOCK_IDS)]\n",
    "    cand = cand[~cand[\"ProductID\"].isin(bought)]\n",
    "    cand = cand[cand[\"FamilyLevel2\"].astype(str).isin(list(map(str, pred_fams)))].copy()\n",
    "\n",
    "    # If empty: fallback to popularity in stock\n",
    "    if cand.empty:\n",
    "        pop = transactions.groupby(\"ProductID\")[\"Quantity\"].sum().sort_values(ascending=False)\n",
    "        cand = products[products[\"ProductID\"].isin(IN_STOCK_IDS)].copy()\n",
    "        cand = cand[~cand[\"ProductID\"].isin(bought)]\n",
    "        cand[\"pop_score\"] = cand[\"ProductID\"].map(pop).fillna(0.0)\n",
    "        cand[\"content_key\"] = make_content_key(cand)\n",
    "        if dedup_by_content:\n",
    "            cand = cand.sort_values(\"pop_score\", ascending=False).drop_duplicates(\"content_key\", keep=\"first\")\n",
    "        return cand.sort_values(\"pop_score\", ascending=False).head(top_k)[\n",
    "            [\"ProductID\"] + FEATURES_HIER + [\"content_key\", \"pop_score\"]\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    # Add content_key\n",
    "    cand[\"content_key\"] = make_content_key(cand)\n",
    "\n",
    "    # XGB family probability (same prob for all products within that family)\n",
    "    fam_prob = {str(f): float(p) for f, p in fam_preds}\n",
    "    cand[\"xgb_family_prob\"] = cand[\"FamilyLevel2\"].astype(str).map(fam_prob).fillna(0.0)\n",
    "\n",
    "    # Profile boost\n",
    "    prof = build_client_profile(cid)\n",
    "    cand[\"profile_boost\"] = profile_boost(prof, cand)\n",
    "\n",
    "    # Content similarity score (aggregate from recent seed products)\n",
    "    hist = transactions[transactions[\"ClientID\"] == cid].sort_values(\"SaleTransactionDate\")\n",
    "    recent = hist[\"ProductID\"].astype(str).tail(50).tolist()\n",
    "\n",
    "    # pick most recent unique seeds\n",
    "    seeds = []\n",
    "    seen = set()\n",
    "    for pid in reversed(recent):\n",
    "        if pid in seen:\n",
    "            continue\n",
    "        seen.add(pid)\n",
    "        seeds.append(pid)\n",
    "        if len(seeds) == n_seed_products:\n",
    "            break\n",
    "\n",
    "    sim_acc = {}\n",
    "    for s in seeds:\n",
    "        recs = content_candidates_from_seed(s, per_seed_k=per_seed_k)\n",
    "        for _, r in recs.iterrows():\n",
    "            pid = str(r[\"ProductID\"])\n",
    "            sim_acc[pid] = sim_acc.get(pid, 0.0) + float(r[\"sim_score\"])\n",
    "\n",
    "    cand[\"content_sim\"] = cand[\"ProductID\"].map(sim_acc).fillna(0.0)\n",
    "\n",
    "    # Final score (tunable)\n",
    "    cand[\"final_score\"] = (\n",
    "        2.0 * cand[\"xgb_family_prob\"] +\n",
    "        1.0 * cand[\"profile_boost\"] +\n",
    "        1.0 * cand[\"content_sim\"]\n",
    "    )\n",
    "\n",
    "    cand = cand.sort_values(\"final_score\", ascending=False)\n",
    "\n",
    "    # Deduplicate by \"content\" at recommendation time\n",
    "    if dedup_by_content:\n",
    "        cand = cand.drop_duplicates(\"content_key\", keep=\"first\")\n",
    "\n",
    "    return cand.head(top_k)[\n",
    "        [\"ProductID\"] + FEATURES_HIER + [\"content_key\", \"xgb_family_prob\", \"profile_boost\", \"content_sim\", \"final_score\"]\n",
    "    ].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f9b06d-ee91-4440-8461-84c8d0c51163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example client: 8119209481417068505\n",
      "\n",
      "Top predicted next families:\n",
      "[('Adidas Rugby Shorts', 0.08971345), ('Canterbury Advantage', 0.06792515), ('Spalding NBA Official Game Ball', 0.06647961), ('Adidas Torpedo', 0.0469231), ('Canterbury Vapodri', 0.045692395)]\n",
      "\n",
      "Recommendations:\n",
      "             ProductID    Category FamilyLevel1 Universe  \\\n",
      "0  7813157068856617209  Basketball         Ball      Men   \n",
      "1  5840373649174391597  Basketball         Ball    Women   \n",
      "2  8706332538369958162       Rugby       Shorts      Men   \n",
      "3  1110391261318430111       Rugby       Shorts    Women   \n",
      "4  3715889193417410749       Rugby       Shorts    Women   \n",
      "5  1684662842964204861       Rugby         Ball      Men   \n",
      "6   950317252666201704       Rugby       Shorts      Men   \n",
      "7   314618247339651600       Rugby         Ball    Women   \n",
      "8  7042499222040039915       Rugby       Jersey      Men   \n",
      "9  4552681534700227474       Rugby       Jersey    Women   \n",
      "\n",
      "                      FamilyLevel2  \\\n",
      "0  Spalding NBA Official Game Ball   \n",
      "1  Spalding NBA Official Game Ball   \n",
      "2              Adidas Rugby Shorts   \n",
      "3              Adidas Rugby Shorts   \n",
      "4             Canterbury Advantage   \n",
      "5                   Adidas Torpedo   \n",
      "6             Canterbury Advantage   \n",
      "7                   Adidas Torpedo   \n",
      "8               Canterbury Vapodri   \n",
      "9               Canterbury Vapodri   \n",
      "\n",
      "                                         content_key  xgb_family_prob  \\\n",
      "0  Basketball | Ball | Spalding NBA Official Game...         0.066480   \n",
      "1  Basketball | Ball | Spalding NBA Official Game...         0.066480   \n",
      "2         Rugby | Shorts | Adidas Rugby Shorts | Men         0.089713   \n",
      "3       Rugby | Shorts | Adidas Rugby Shorts | Women         0.089713   \n",
      "4      Rugby | Shorts | Canterbury Advantage | Women         0.067925   \n",
      "5                Rugby | Ball | Adidas Torpedo | Men         0.046923   \n",
      "6        Rugby | Shorts | Canterbury Advantage | Men         0.067925   \n",
      "7              Rugby | Ball | Adidas Torpedo | Women         0.046923   \n",
      "8          Rugby | Jersey | Canterbury Vapodri | Men         0.045692   \n",
      "9        Rugby | Jersey | Canterbury Vapodri | Women         0.045692   \n",
      "\n",
      "   profile_boost  content_sim  final_score  \n",
      "0       0.254249     1.000000     1.387208  \n",
      "1       0.220961     0.973325     1.327246  \n",
      "2       0.083736     0.972516     1.235678  \n",
      "3       0.050448     1.000000     1.229875  \n",
      "4       0.052542     0.804513     0.992905  \n",
      "5       0.137232     0.000000     0.231079  \n",
      "6       0.085830     0.000000     0.221680  \n",
      "7       0.103945     0.000000     0.197791  \n",
      "8       0.066608     0.000000     0.157993  \n",
      "9       0.033320     0.000000     0.124705  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/wq8vt3b9487397c3mf0t_s980000gn/T/ipykernel_4383/3936920477.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  s = df.groupby(feat).apply(lambda g: float(base_w.loc[g.index].sum()))\n",
      "/var/folders/85/wq8vt3b9487397c3mf0t_s980000gn/T/ipykernel_4383/3936920477.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  s = df.groupby(feat).apply(lambda g: float(base_w.loc[g.index].sum()))\n",
      "/var/folders/85/wq8vt3b9487397c3mf0t_s980000gn/T/ipykernel_4383/3936920477.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  s = df.groupby(feat).apply(lambda g: float(base_w.loc[g.index].sum()))\n",
      "/var/folders/85/wq8vt3b9487397c3mf0t_s980000gn/T/ipykernel_4383/3936920477.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  s = df.groupby(feat).apply(lambda g: float(base_w.loc[g.index].sum()))\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 10) Demo: run for one client\n",
    "# ---------------------------\n",
    "example_client = transactions[\"ClientID\"].iloc[0]\n",
    "print(\"Example client:\", example_client)\n",
    "\n",
    "print(\"\\nTop predicted next families:\")\n",
    "print(predict_top_families_for_client(example_client, top_n=5))\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(recommend_for_client(example_client, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8bef6d-08a3-4184-9daf-5793ce1736c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Streamlit bundle to: streamlit_model_bundle\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib\n",
    "\n",
    "BUNDLE_DIR = \"streamlit_model_bundle\"\n",
    "os.makedirs(BUNDLE_DIR, exist_ok=True)\n",
    "\n",
    "def save_streamlit_bundle(bundle_dir=BUNDLE_DIR):\n",
    "    # --- 1) Prediction model ---\n",
    "    joblib.dump(clf, f\"{bundle_dir}/xgb_pipeline.joblib\")\n",
    "    joblib.dump(le, f\"{bundle_dir}/label_encoder.joblib\")\n",
    "\n",
    "    # --- 2) Content model ---\n",
    "    joblib.dump(products_model, f\"{bundle_dir}/products_model.joblib\")\n",
    "    joblib.dump(S, f\"{bundle_dir}/similarity_matrix.joblib\")\n",
    "    joblib.dump(product_index_model, f\"{bundle_dir}/product_index.joblib\")\n",
    "\n",
    "    # --- 3) Reference data for app ---\n",
    "    # Keep these smaller if you can; Streamlit can load big files but slower.\n",
    "    joblib.dump(products, f\"{bundle_dir}/products.joblib\")\n",
    "    joblib.dump(transactions, f\"{bundle_dir}/transactions.joblib\")\n",
    "    if stocks is not None:\n",
    "        joblib.dump(stocks, f\"{bundle_dir}/stocks.joblib\")\n",
    "    else:\n",
    "        joblib.dump(list(IN_STOCK_IDS), f\"{bundle_dir}/in_stock_ids.joblib\")\n",
    "\n",
    "    # --- 4) Feature lists & params needed at inference ---\n",
    "    artifacts = {\n",
    "        \"num_features\": num_features,\n",
    "        \"cat_features\": cat_features,\n",
    "        \"top_fam1\": top_fam1,\n",
    "        \"features_hier\": FEATURES_HIER,\n",
    "        \"half_life_days\": HALF_LIFE_DAYS,\n",
    "        \"profile_weights\": W_PROFILE,\n",
    "        \"content_weights\": content_model.get(\"weights\", {}),\n",
    "        # final scoring weights (match your recommender)\n",
    "        \"final_score_weights\": {\"xgb_family_prob\": 2.0, \"profile_boost\": 1.0, \"content_sim\": 1.0},\n",
    "        \"num_class\": int(len(le.classes_)),\n",
    "    }\n",
    "\n",
    "    with open(f\"{bundle_dir}/artifacts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(artifacts, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved Streamlit bundle to: {bundle_dir}\")\n",
    "\n",
    "save_streamlit_bundle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19be7bc-c61a-4352-b683-27baf46cd66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
